{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e60e47dc",
   "metadata": {},
   "source": [
    "## Chat Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a9a9f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5080f1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Je aime le programmation.', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-05-09T06:47:45.65063692Z', 'done': True, 'done_reason': 'stop', 'total_duration': 11814008799, 'load_duration': 5626504578, 'prompt_eval_count': 45, 'prompt_eval_duration': 4919000000, 'eval_count': 7, 'eval_duration': 1263000000, 'model_name': 'llama3.2'}, id='run-17ed2c50-aa32-41e6-81af-386b32a5358f-0', usage_metadata={'input_tokens': 45, 'output_tokens': 7, 'total_tokens': 52})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b5cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28139f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ich liebe Programmierung.', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-05-09T06:47:48.821932025Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3033537819, 'load_duration': 29819520, 'prompt_eval_count': 40, 'prompt_eval_duration': 1938000000, 'eval_count': 6, 'eval_duration': 1064000000, 'model_name': 'llama3.2'}, id='run-080ef155-7ba4-4ae4-87ce-67588f378e25-0', usage_metadata={'input_tokens': 40, 'output_tokens': 6, 'total_tokens': 46})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\",\n",
    "        \"output_language\": \"German\",\n",
    "        \"input\": \"I love programming.\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e620c04",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/tutorials/llm_chain/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7ec13d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that answers to the query using the following context. \\nContext: {context}\",\n",
    "        ),\n",
    "        (\"user\", \"{query}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "985c2a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = prompt.invoke({\"context\": \"Vasim is an engineer\", \"query\": \"who is vasim\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ecb7061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "659d924d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't have any specific information about a person named Vasim, but I can tell you that there may be multiple individuals with this name.\\n\\nHowever, based on our initial conversation, it seems that Vasim is an engineer. If you could provide more context or details about who Vasim is or what he does as an engineer, I'd be happy to try and help further!\", additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-05-09T06:48:12.781348351Z', 'done': True, 'done_reason': 'stop', 'total_duration': 23880290262, 'load_duration': 51722476, 'prompt_eval_count': 52, 'prompt_eval_duration': 3961000000, 'eval_count': 78, 'eval_duration': 19865000000, 'model_name': 'llama3.2'}, id='run-d1559d60-f797-490f-b9c3-d945700c9cd3-0', usage_metadata={'input_tokens': 52, 'output_tokens': 78, 'total_tokens': 130})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"context\": \"Vasim is an engineer\",\n",
    "        \"query\": \"who is vasim\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bd1774",
   "metadata": {},
   "source": [
    "## Groq LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33f03dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04611e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f1d0258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The translation of \"I love programming\" to French is:\\n\\n\"J\\'adore le programmation.\"', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 55, 'total_tokens': 77, 'completion_time': 0.029333333, 'prompt_time': 0.005692778, 'queue_time': 0.20517924199999998, 'total_time': 0.035026111}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f7bd09b454', 'finish_reason': 'stop', 'logprobs': None}, id='run-3d75f415-9b2e-4ee4-b0de-1a8a4bacd219-0', usage_metadata={'input_tokens': 55, 'output_tokens': 22, 'total_tokens': 77})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d983a4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ich liebe Programmieren.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 50, 'total_tokens': 56, 'completion_time': 0.008, 'prompt_time': 0.003717056, 'queue_time': 0.168124819, 'total_time': 0.011717056}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f7bd09b454', 'finish_reason': 'stop', 'logprobs': None}, id='run-c3d30253-ca9e-4fc4-99c6-2b581e4c1ae2-0', usage_metadata={'input_tokens': 50, 'output_tokens': 6, 'total_tokens': 56})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\",\n",
    "        \"output_language\": \"German\",\n",
    "        \"input\": \"I love programming.\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fb3e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ollama_model(\n",
    "        model_name: str = \"llama3.2\",\n",
    "        temperature: float = 0,\n",
    "        max_tokens: int = 100,\n",
    "):\n",
    "    return ChatOllama(\n",
    "        model=model_name,\n",
    "        temperature=temperature\n",
    "        # other params...\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b949d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_groq_model(\n",
    "        model_name: str = \"llama-3.1-8b-instant\",\n",
    "        temperature: float = 0,\n",
    "        max_tokens: int = None,\n",
    "        timeout: int = None,\n",
    "        max_retries: int = 2,\n",
    "):\n",
    "    return ChatGroq(\n",
    "        model=model_name,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        timeout=timeout,\n",
    "        max_retries=max_retries,\n",
    "        # other params...\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
